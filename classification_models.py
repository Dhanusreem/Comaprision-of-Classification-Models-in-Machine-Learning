# -*- coding: utf-8 -*-
"""Classification_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b1rm9u_QGxITW3fF79LVNNJnDaGJW8S9

IMPROTING READING AND ASSIGNING
"""

#importing the neccesary libraary
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('bmi.csv')
df.head()
# assigning the Index column as the target
target = 'Index'
# the categorical value for the index
labels = ['Extremely Weak','Weak','Normal','Overweight','Obsesity','Extreme Obesity']
#so that feature contains all columns except the target variable
features = [i for i in df.columns.values if i not in [target]]

"""DATA-VISUALIZATION"""

label_map = {0: 'Extremely Weak', 1: 'Weak', 2: 'Normal', 3: 'Overweight', 4: 'Obsesity', 5: 'Extreme Obesity'}
# maping the original values into the index column
df[target] = df[target].map(label_map)
value_counts = df[target].value_counts()
# Plotting the pie chart
plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=180)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Distribution of Target Variable') # so the titl is displayed
plt.show()

col_name = 'Gender'
value_counts = df[col_name].value_counts()

# Plotting the bar chart
plt.figure(figsize=(10, 6))
value_counts.plot(kind='bar', color='pink')
plt.title(f'Bar Plot of {col_name}')
plt.xlabel(col_name)
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()  #
plt.show()

"""DATA PREPROCESSING"""

df.isnull().sum()/len(df) *100

df = df.drop_duplicates()

"""LABEL ENCODING

"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Gender']= le.fit_transform(df['Gender'])
print(df)

df.duplicated().sum()

"""DATA SPLITTING AND TRAING

"""

X = df.iloc[:, :-1].values #select all rows and all columns except the last one
y = df.iloc[:, -1].values #  select all rows and only the last column
# selecting features(x) and target(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train) #computes mean,standard deviation for each feature in the training data
X_train = scaler.transform(X_train) #  The transform method standardizes each feature in X_train by subtracting the mean and dividing by the standard deviation.
X_test = scaler.transform(X_test)
#X: The feature matrix (input variables),y: The target vector (output variable).

"""GINI(75) V/S ENTROPY(79)

"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(max_depth = 25, min_samples_leaf=5) # criterion = 'entropy'

dt.fit(X_train,y_train)

y_pred = dt.predict(X_test)
# predictions for the test data, you can compare these predictions (y_pred) with the actual class labels (y_test) to evaluate the performance of the decision tree model. Common metrics for this evaluation include accuracy, precision, recall, and F1-score.

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
cr = classification_report(y_test, y_pred)
print("\nClassification Report:",)
print (cr)
acc = accuracy_score(y_test,y_pred)
print("\nAccuracy:",acc)

from sklearn import tree
plt.figure(figsize=(15,10)) # new figure for the plot with a specified size of 15 inches by 10 inches. The figsize parameter is used to control the dimensions of the plot, making it large enough to display the decision tree clearly
tree.plot_tree(dt,filled=True) #  fills the nodes with colors that represent the class they predict. The intensity of the color indicates the proportion of samples in the node that belong to the predicted class.
plt.show()

dt1 = DecisionTreeClassifier(max_depth = 25,min_samples_leaf=5, criterion = 'entropy')
dt1.fit(X_train,y_train)

y_pred1 = dt1.predict(X_test)

cm = confusion_matrix(y_test, y_pred1)# It shows the counts of true positive, true negative, false positive, and false negative predictions.
print("Confusion Matrix:")
print(cm)
cr = classification_report(y_test, y_pred1)#the classsifiavtion gives the summary of the models perfoemance
print("\nClassification Report:",)
print (cr)
acc = accuracy_score(y_test,y_pred1)
print("\nAccuracy:",acc)

plt.figure(figsize=(15,10))
tree.plot_tree(dt1,filled=True)
plt.show()

"""Random Forest(82)"""

# Data Processing
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

#Load the Dataset
customer = pd.read_csv("bmi.csv")
customer.head()

# Split into train and test
from sklearn.model_selection import train_test_split, cross_val_score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)# It ensures that the data split will be the same each time the code is run

#Train a machine learning model
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(random_state=42))#ensemble model tht fits the decision tree.... ensuring that the random number generation used in the classifier is consistent.
])

pipeline.fit(X_train, y_train)

# Modelling
# Random forest Classifier
clf = RandomForestClassifier(n_estimators = 20) # Random Forest with 20 trees

clf.fit(X, y) # It can only handle numerical attributes!

from sklearn.metrics import classification_report

# Make prediction on the testing data
y_pred = pipeline.predict(X_test)

# Classification Report
print(classification_report(y_pred, y_test))